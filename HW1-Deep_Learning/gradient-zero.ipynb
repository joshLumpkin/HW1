{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d8bc717-6f12-4cdc-9bc8-48814171a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474947c-1c8e-4468-895f-775053cf1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always using CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate inputs and outputs\n",
    "x = torch.linspace(-1, 1, 1000).view(-1, 1)\n",
    "y = torch.sign(torch.sin(5 * np.pi * x))\n",
    "\n",
    "# dataset and loader\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd45c3bb-0378-4131-a1f4-c06a17e35b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple modal\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c812a00-7816-4b90-9c9f-8324f6ccfc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45489a44-ead7-4a8b-9ae2-11331774afea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1, Loss: 1.0936, Gradient Norm: 1.6477, Minimal Ratio: 0.0018\n",
      "Epoch: 1, Batch: 11, Loss: 0.6315, Gradient Norm: 0.3835, Minimal Ratio: 0.0072\n",
      "Epoch: 1, Batch: 21, Loss: 0.3774, Gradient Norm: 0.4126, Minimal Ratio: 0.0069\n",
      "Epoch: 1, Batch: 31, Loss: 0.2364, Gradient Norm: 0.2618, Minimal Ratio: 0.0084\n",
      "Epoch: 2, Batch: 1, Loss: -0.3352, Gradient Norm: 1.0638, Minimal Ratio: 0.0025\n",
      "Epoch: 2, Batch: 11, Loss: -0.8863, Gradient Norm: 1.3462, Minimal Ratio: 0.0026\n",
      "Epoch: 2, Batch: 21, Loss: -0.4768, Gradient Norm: 0.7189, Minimal Ratio: 0.0049\n",
      "Epoch: 2, Batch: 31, Loss: -0.7216, Gradient Norm: 0.9320, Minimal Ratio: 0.0034\n",
      "Epoch: 3, Batch: 1, Loss: -1.4473, Gradient Norm: 1.6776, Minimal Ratio: 0.0023\n",
      "Epoch: 3, Batch: 11, Loss: -0.6981, Gradient Norm: 0.9877, Minimal Ratio: 0.0039\n",
      "Epoch: 3, Batch: 21, Loss: 0.2556, Gradient Norm: 0.5149, Minimal Ratio: 0.0102\n",
      "Epoch: 3, Batch: 31, Loss: -1.1616, Gradient Norm: 1.0794, Minimal Ratio: 0.0046\n",
      "Epoch: 4, Batch: 1, Loss: 3.0486, Gradient Norm: 2.4334, Minimal Ratio: 0.0007\n",
      "Epoch: 4, Batch: 11, Loss: -3.5678, Gradient Norm: 2.8643, Minimal Ratio: 0.0014\n",
      "Epoch: 4, Batch: 21, Loss: -1.4888, Gradient Norm: 0.7571, Minimal Ratio: 0.0077\n",
      "Epoch: 4, Batch: 31, Loss: -3.9122, Gradient Norm: 1.1666, Minimal Ratio: 0.0067\n",
      "Epoch: 5, Batch: 1, Loss: -4.4743, Gradient Norm: 1.9530, Minimal Ratio: 0.0025\n",
      "Epoch: 5, Batch: 11, Loss: -2.9965, Gradient Norm: 1.3239, Minimal Ratio: 0.0051\n",
      "Epoch: 5, Batch: 21, Loss: 0.4003, Gradient Norm: 1.9179, Minimal Ratio: 0.0029\n",
      "Epoch: 5, Batch: 31, Loss: 2.0851, Gradient Norm: 0.8375, Minimal Ratio: 0.0065\n",
      "Epoch: 6, Batch: 1, Loss: -3.8136, Gradient Norm: 2.0512, Minimal Ratio: 0.0026\n",
      "Epoch: 6, Batch: 11, Loss: -4.6764, Gradient Norm: 0.9844, Minimal Ratio: 0.0072\n",
      "Epoch: 6, Batch: 21, Loss: -3.3380, Gradient Norm: 0.5121, Minimal Ratio: 0.0118\n",
      "Epoch: 6, Batch: 31, Loss: 1.3469, Gradient Norm: 1.9960, Minimal Ratio: 0.0037\n",
      "Epoch: 7, Batch: 1, Loss: 2.4266, Gradient Norm: 1.7339, Minimal Ratio: 0.0030\n",
      "Epoch: 7, Batch: 11, Loss: -2.1031, Gradient Norm: 0.9574, Minimal Ratio: 0.0056\n",
      "Epoch: 7, Batch: 21, Loss: -3.2546, Gradient Norm: 2.0954, Minimal Ratio: 0.0027\n",
      "Epoch: 7, Batch: 31, Loss: 0.3549, Gradient Norm: 0.7582, Minimal Ratio: 0.0067\n",
      "Epoch: 8, Batch: 1, Loss: 3.4545, Gradient Norm: 2.5966, Minimal Ratio: 0.0022\n",
      "Epoch: 8, Batch: 11, Loss: 0.3138, Gradient Norm: 0.8698, Minimal Ratio: 0.0058\n",
      "Epoch: 8, Batch: 21, Loss: 0.2141, Gradient Norm: 2.8092, Minimal Ratio: 0.0019\n",
      "Epoch: 8, Batch: 31, Loss: -1.9219, Gradient Norm: 0.5941, Minimal Ratio: 0.0119\n",
      "Epoch: 9, Batch: 1, Loss: -3.2652, Gradient Norm: 1.8365, Minimal Ratio: 0.0031\n",
      "Epoch: 9, Batch: 11, Loss: 0.1940, Gradient Norm: 1.1406, Minimal Ratio: 0.0041\n",
      "Epoch: 9, Batch: 21, Loss: -9.2986, Gradient Norm: 1.2251, Minimal Ratio: 0.0041\n",
      "Epoch: 9, Batch: 31, Loss: 1.6251, Gradient Norm: 1.5121, Minimal Ratio: 0.0036\n",
      "Epoch: 10, Batch: 1, Loss: 4.9238, Gradient Norm: 1.2001, Minimal Ratio: 0.0046\n",
      "Epoch: 10, Batch: 11, Loss: 0.2787, Gradient Norm: 2.5889, Minimal Ratio: 0.0020\n",
      "Epoch: 10, Batch: 21, Loss: 2.2988, Gradient Norm: 0.6949, Minimal Ratio: 0.0128\n",
      "Epoch: 10, Batch: 31, Loss: -7.8255, Gradient Norm: 1.1448, Minimal Ratio: 0.0059\n",
      "Epoch: 11, Batch: 1, Loss: 1.0371, Gradient Norm: 1.0483, Minimal Ratio: 0.0038\n",
      "Epoch: 11, Batch: 11, Loss: -3.6825, Gradient Norm: 1.9849, Minimal Ratio: 0.0028\n",
      "Epoch: 11, Batch: 21, Loss: 2.0783, Gradient Norm: 3.4132, Minimal Ratio: 0.0002\n",
      "Epoch: 11, Batch: 31, Loss: -3.5623, Gradient Norm: 3.5787, Minimal Ratio: 0.0014\n",
      "Epoch: 12, Batch: 1, Loss: -5.0170, Gradient Norm: 3.9515, Minimal Ratio: 0.0014\n",
      "Epoch: 12, Batch: 11, Loss: 1.6417, Gradient Norm: 2.3494, Minimal Ratio: 0.0026\n",
      "Epoch: 12, Batch: 21, Loss: 1.9205, Gradient Norm: 2.2386, Minimal Ratio: 0.0027\n",
      "Epoch: 12, Batch: 31, Loss: 0.5586, Gradient Norm: 2.7568, Minimal Ratio: 0.0020\n",
      "Epoch: 13, Batch: 1, Loss: -2.1305, Gradient Norm: 2.0609, Minimal Ratio: 0.0024\n",
      "Epoch: 13, Batch: 11, Loss: 1.1646, Gradient Norm: 1.9724, Minimal Ratio: 0.0025\n",
      "Epoch: 13, Batch: 21, Loss: 4.4301, Gradient Norm: 2.8590, Minimal Ratio: 0.0019\n",
      "Epoch: 13, Batch: 31, Loss: -1.9078, Gradient Norm: 2.2196, Minimal Ratio: 0.0026\n",
      "Epoch: 14, Batch: 1, Loss: -3.6840, Gradient Norm: 3.5957, Minimal Ratio: 0.0015\n",
      "Epoch: 14, Batch: 11, Loss: -6.2029, Gradient Norm: 3.1533, Minimal Ratio: 0.0016\n",
      "Epoch: 14, Batch: 21, Loss: -3.7852, Gradient Norm: 1.1100, Minimal Ratio: 0.0065\n",
      "Epoch: 14, Batch: 31, Loss: -3.0930, Gradient Norm: 1.2130, Minimal Ratio: 0.0055\n",
      "Epoch: 15, Batch: 1, Loss: -3.5809, Gradient Norm: 2.3091, Minimal Ratio: 0.0021\n",
      "Epoch: 15, Batch: 11, Loss: 2.4153, Gradient Norm: 1.5182, Minimal Ratio: 0.0041\n",
      "Epoch: 15, Batch: 21, Loss: 0.6548, Gradient Norm: 2.4247, Minimal Ratio: 0.0025\n",
      "Epoch: 15, Batch: 31, Loss: -2.3540, Gradient Norm: 1.0537, Minimal Ratio: 0.0039\n",
      "Epoch: 16, Batch: 1, Loss: -5.5233, Gradient Norm: 1.4004, Minimal Ratio: 0.0043\n",
      "Epoch: 16, Batch: 11, Loss: -6.7235, Gradient Norm: 1.2384, Minimal Ratio: 0.0027\n",
      "Epoch: 16, Batch: 21, Loss: -5.0560, Gradient Norm: 1.9204, Minimal Ratio: 0.0025\n",
      "Epoch: 16, Batch: 31, Loss: -3.3805, Gradient Norm: 2.2971, Minimal Ratio: 0.0023\n",
      "Epoch: 17, Batch: 1, Loss: -1.2399, Gradient Norm: 1.4790, Minimal Ratio: 0.0038\n",
      "Epoch: 17, Batch: 11, Loss: -3.2226, Gradient Norm: 0.7928, Minimal Ratio: 0.0066\n",
      "Epoch: 17, Batch: 21, Loss: -4.3913, Gradient Norm: 0.5985, Minimal Ratio: 0.0110\n",
      "Epoch: 17, Batch: 31, Loss: -2.1888, Gradient Norm: 0.4956, Minimal Ratio: 0.0081\n",
      "Epoch: 18, Batch: 1, Loss: -5.8450, Gradient Norm: 1.8211, Minimal Ratio: 0.0032\n",
      "Epoch: 18, Batch: 11, Loss: -2.4735, Gradient Norm: 2.1945, Minimal Ratio: 0.0028\n",
      "Epoch: 18, Batch: 21, Loss: -6.2279, Gradient Norm: 2.5171, Minimal Ratio: 0.0020\n",
      "Epoch: 18, Batch: 31, Loss: -0.3536, Gradient Norm: 2.4702, Minimal Ratio: 0.0027\n",
      "Epoch: 19, Batch: 1, Loss: -2.0037, Gradient Norm: 0.8186, Minimal Ratio: 0.0066\n",
      "Epoch: 19, Batch: 11, Loss: 0.2752, Gradient Norm: 1.0543, Minimal Ratio: 0.0061\n",
      "Epoch: 19, Batch: 21, Loss: 2.5623, Gradient Norm: 2.3743, Minimal Ratio: 0.0023\n",
      "Epoch: 19, Batch: 31, Loss: 4.2288, Gradient Norm: 2.7265, Minimal Ratio: 0.0022\n",
      "Epoch: 20, Batch: 1, Loss: -2.5876, Gradient Norm: 2.5413, Minimal Ratio: 0.0019\n",
      "Epoch: 20, Batch: 11, Loss: 0.7084, Gradient Norm: 0.9820, Minimal Ratio: 0.0077\n",
      "Epoch: 20, Batch: 21, Loss: -9.4845, Gradient Norm: 1.9714, Minimal Ratio: 0.0028\n",
      "Epoch: 20, Batch: 31, Loss: 6.3683, Gradient Norm: 4.9338, Minimal Ratio: 0.0011\n",
      "Epoch: 21, Batch: 1, Loss: 0.8176, Gradient Norm: 0.6101, Minimal Ratio: 0.0065\n",
      "Epoch: 21, Batch: 11, Loss: -0.9501, Gradient Norm: 1.8950, Minimal Ratio: 0.0033\n",
      "Epoch: 21, Batch: 21, Loss: -10.9169, Gradient Norm: 2.0520, Minimal Ratio: 0.0028\n",
      "Epoch: 21, Batch: 31, Loss: -3.7571, Gradient Norm: 2.4454, Minimal Ratio: 0.0030\n",
      "Epoch: 22, Batch: 1, Loss: -6.9806, Gradient Norm: 1.2085, Minimal Ratio: 0.0052\n",
      "Epoch: 22, Batch: 11, Loss: 1.7604, Gradient Norm: 3.8109, Minimal Ratio: 0.0015\n",
      "Epoch: 22, Batch: 21, Loss: -3.3462, Gradient Norm: 1.3152, Minimal Ratio: 0.0027\n",
      "Epoch: 22, Batch: 31, Loss: -9.5809, Gradient Norm: 3.2974, Minimal Ratio: 0.0016\n",
      "Epoch: 23, Batch: 1, Loss: -0.2801, Gradient Norm: 0.7807, Minimal Ratio: 0.0072\n",
      "Epoch: 23, Batch: 11, Loss: -3.3630, Gradient Norm: 0.6054, Minimal Ratio: 0.0107\n",
      "Epoch: 23, Batch: 21, Loss: -1.4784, Gradient Norm: 1.7860, Minimal Ratio: 0.0025\n",
      "Epoch: 23, Batch: 31, Loss: -2.4402, Gradient Norm: 3.1223, Minimal Ratio: 0.0015\n",
      "Epoch: 24, Batch: 1, Loss: -1.3513, Gradient Norm: 1.1488, Minimal Ratio: 0.0038\n",
      "Epoch: 24, Batch: 11, Loss: 1.2079, Gradient Norm: 1.7048, Minimal Ratio: 0.0029\n",
      "Epoch: 24, Batch: 21, Loss: -0.3064, Gradient Norm: 0.8873, Minimal Ratio: 0.0049\n",
      "Epoch: 24, Batch: 31, Loss: 5.5128, Gradient Norm: 5.9086, Minimal Ratio: 0.0009\n",
      "Epoch: 25, Batch: 1, Loss: -0.2932, Gradient Norm: 2.1232, Minimal Ratio: 0.0019\n",
      "Epoch: 25, Batch: 11, Loss: -10.9978, Gradient Norm: 4.6234, Minimal Ratio: 0.0011\n",
      "Epoch: 25, Batch: 21, Loss: 6.9649, Gradient Norm: 4.2824, Minimal Ratio: 0.0014\n",
      "Epoch: 25, Batch: 31, Loss: -0.7729, Gradient Norm: 0.9631, Minimal Ratio: 0.0043\n",
      "Epoch: 26, Batch: 1, Loss: -7.2929, Gradient Norm: 3.6720, Minimal Ratio: 0.0017\n",
      "Epoch: 26, Batch: 11, Loss: -0.5184, Gradient Norm: 3.2490, Minimal Ratio: 0.0016\n",
      "Epoch: 26, Batch: 21, Loss: -11.0554, Gradient Norm: 1.7794, Minimal Ratio: 0.0035\n",
      "Epoch: 26, Batch: 31, Loss: -1.9884, Gradient Norm: 2.0417, Minimal Ratio: 0.0029\n",
      "Epoch: 27, Batch: 1, Loss: -4.5634, Gradient Norm: 2.4810, Minimal Ratio: 0.0020\n",
      "Epoch: 27, Batch: 11, Loss: 1.3403, Gradient Norm: 3.9373, Minimal Ratio: 0.0013\n",
      "Epoch: 27, Batch: 21, Loss: -0.0595, Gradient Norm: 2.0475, Minimal Ratio: 0.0029\n",
      "Epoch: 27, Batch: 31, Loss: -8.5274, Gradient Norm: 1.5133, Minimal Ratio: 0.0036\n",
      "Epoch: 28, Batch: 1, Loss: 0.4722, Gradient Norm: 2.3453, Minimal Ratio: 0.0030\n",
      "Epoch: 28, Batch: 11, Loss: 0.3018, Gradient Norm: 0.8382, Minimal Ratio: 0.0039\n",
      "Epoch: 28, Batch: 21, Loss: -0.0537, Gradient Norm: 3.5089, Minimal Ratio: 0.0013\n",
      "Epoch: 28, Batch: 31, Loss: -8.1201, Gradient Norm: 5.0093, Minimal Ratio: 0.0011\n",
      "Epoch: 29, Batch: 1, Loss: -1.7505, Gradient Norm: 1.8896, Minimal Ratio: 0.0020\n",
      "Epoch: 29, Batch: 11, Loss: -8.6907, Gradient Norm: 3.5814, Minimal Ratio: 0.0015\n",
      "Epoch: 29, Batch: 21, Loss: -3.0692, Gradient Norm: 5.0580, Minimal Ratio: 0.0011\n",
      "Epoch: 29, Batch: 31, Loss: -4.5062, Gradient Norm: 1.1054, Minimal Ratio: 0.0148\n",
      "Epoch: 30, Batch: 1, Loss: 6.5907, Gradient Norm: 0.7066, Minimal Ratio: 0.0075\n",
      "Epoch: 30, Batch: 11, Loss: -7.7416, Gradient Norm: 2.5276, Minimal Ratio: 0.0018\n",
      "Epoch: 30, Batch: 21, Loss: -2.7864, Gradient Norm: 1.7427, Minimal Ratio: 0.0016\n",
      "Epoch: 30, Batch: 31, Loss: -6.5743, Gradient Norm: 1.5814, Minimal Ratio: 0.0006\n",
      "Epoch: 31, Batch: 1, Loss: -0.2617, Gradient Norm: 3.2271, Minimal Ratio: 0.0010\n",
      "Epoch: 31, Batch: 11, Loss: -3.0990, Gradient Norm: 0.9301, Minimal Ratio: 0.0053\n",
      "Epoch: 31, Batch: 21, Loss: 4.0662, Gradient Norm: 1.0127, Minimal Ratio: 0.0022\n",
      "Epoch: 31, Batch: 31, Loss: -1.1354, Gradient Norm: 1.3478, Minimal Ratio: 0.0030\n",
      "Epoch: 32, Batch: 1, Loss: -4.5874, Gradient Norm: 1.6347, Minimal Ratio: 0.0020\n",
      "Epoch: 32, Batch: 11, Loss: 1.3929, Gradient Norm: 1.2994, Minimal Ratio: 0.0037\n",
      "Epoch: 32, Batch: 21, Loss: -2.7601, Gradient Norm: 1.5756, Minimal Ratio: 0.0024\n",
      "Epoch: 32, Batch: 31, Loss: -0.9252, Gradient Norm: 3.1497, Minimal Ratio: 0.0018\n",
      "Epoch: 33, Batch: 1, Loss: 1.8517, Gradient Norm: 2.5952, Minimal Ratio: 0.0006\n",
      "Epoch: 33, Batch: 11, Loss: 2.5559, Gradient Norm: 2.5033, Minimal Ratio: 0.0022\n",
      "Epoch: 33, Batch: 21, Loss: -2.2371, Gradient Norm: 2.2907, Minimal Ratio: 0.0009\n",
      "Epoch: 33, Batch: 31, Loss: -4.1192, Gradient Norm: 1.6882, Minimal Ratio: 0.0018\n",
      "Epoch: 34, Batch: 1, Loss: -3.1572, Gradient Norm: 1.1999, Minimal Ratio: 0.0038\n",
      "Epoch: 34, Batch: 11, Loss: -4.5083, Gradient Norm: 3.5703, Minimal Ratio: 0.0016\n",
      "Epoch: 34, Batch: 21, Loss: -2.4216, Gradient Norm: 1.5156, Minimal Ratio: 0.0063\n",
      "Epoch: 34, Batch: 31, Loss: -0.4026, Gradient Norm: 4.8167, Minimal Ratio: 0.0011\n",
      "Epoch: 35, Batch: 1, Loss: -3.2076, Gradient Norm: 2.2064, Minimal Ratio: 0.0029\n",
      "Epoch: 35, Batch: 11, Loss: -2.1551, Gradient Norm: 1.0054, Minimal Ratio: 0.0049\n",
      "Epoch: 35, Batch: 21, Loss: -0.6448, Gradient Norm: 1.0546, Minimal Ratio: 0.0025\n",
      "Epoch: 35, Batch: 31, Loss: -7.1320, Gradient Norm: 1.8948, Minimal Ratio: 0.0023\n",
      "Epoch: 36, Batch: 1, Loss: -0.4436, Gradient Norm: 1.5439, Minimal Ratio: 0.0042\n",
      "Epoch: 36, Batch: 11, Loss: -4.8408, Gradient Norm: 1.1887, Minimal Ratio: 0.0055\n",
      "Epoch: 36, Batch: 21, Loss: -4.4875, Gradient Norm: 1.9889, Minimal Ratio: 0.0033\n",
      "Epoch: 36, Batch: 31, Loss: -1.6514, Gradient Norm: 1.2585, Minimal Ratio: 0.0023\n",
      "Epoch: 37, Batch: 1, Loss: -5.6068, Gradient Norm: 2.0146, Minimal Ratio: 0.0009\n",
      "Epoch: 37, Batch: 11, Loss: -9.1141, Gradient Norm: 3.3764, Minimal Ratio: 0.0012\n",
      "Epoch: 37, Batch: 21, Loss: -1.1436, Gradient Norm: 1.4670, Minimal Ratio: 0.0056\n",
      "Epoch: 37, Batch: 31, Loss: 1.8524, Gradient Norm: 6.7297, Minimal Ratio: 0.0004\n",
      "Epoch: 38, Batch: 1, Loss: 2.7785, Gradient Norm: 7.2473, Minimal Ratio: 0.0004\n",
      "Epoch: 38, Batch: 11, Loss: -1.4177, Gradient Norm: 1.7373, Minimal Ratio: 0.0019\n",
      "Epoch: 38, Batch: 21, Loss: -2.4331, Gradient Norm: 0.9585, Minimal Ratio: 0.0052\n",
      "Epoch: 38, Batch: 31, Loss: -2.4920, Gradient Norm: 1.8740, Minimal Ratio: 0.0017\n",
      "Epoch: 39, Batch: 1, Loss: -6.7001, Gradient Norm: 4.8131, Minimal Ratio: 0.0006\n",
      "Epoch: 39, Batch: 11, Loss: -10.3478, Gradient Norm: 6.5240, Minimal Ratio: 0.0004\n",
      "Epoch: 39, Batch: 21, Loss: -6.7728, Gradient Norm: 2.6516, Minimal Ratio: 0.0024\n",
      "Epoch: 39, Batch: 31, Loss: -0.0293, Gradient Norm: 1.3629, Minimal Ratio: 0.0013\n",
      "Epoch: 40, Batch: 1, Loss: -3.0582, Gradient Norm: 1.2686, Minimal Ratio: 0.0015\n",
      "Epoch: 40, Batch: 11, Loss: -1.1744, Gradient Norm: 2.4062, Minimal Ratio: 0.0014\n",
      "Epoch: 40, Batch: 21, Loss: -0.7504, Gradient Norm: 5.7306, Minimal Ratio: 0.0004\n",
      "Epoch: 40, Batch: 31, Loss: -3.9978, Gradient Norm: 4.2840, Minimal Ratio: 0.0004\n",
      "Epoch: 41, Batch: 1, Loss: 3.4504, Gradient Norm: 3.7584, Minimal Ratio: 0.0009\n",
      "Epoch: 41, Batch: 11, Loss: 2.0162, Gradient Norm: 3.8733, Minimal Ratio: 0.0005\n",
      "Epoch: 41, Batch: 21, Loss: -7.2953, Gradient Norm: 2.9332, Minimal Ratio: 0.0021\n",
      "Epoch: 41, Batch: 31, Loss: 2.6189, Gradient Norm: 4.8468, Minimal Ratio: 0.0024\n",
      "Epoch: 42, Batch: 1, Loss: -1.8208, Gradient Norm: 1.6959, Minimal Ratio: 0.0016\n",
      "Epoch: 42, Batch: 11, Loss: -6.9685, Gradient Norm: 2.5415, Minimal Ratio: 0.0009\n",
      "Epoch: 42, Batch: 21, Loss: 4.3246, Gradient Norm: 3.2614, Minimal Ratio: 0.0019\n",
      "Epoch: 42, Batch: 31, Loss: -6.4000, Gradient Norm: 2.9986, Minimal Ratio: 0.0012\n",
      "Epoch: 43, Batch: 1, Loss: -7.0662, Gradient Norm: 5.1435, Minimal Ratio: 0.0022\n",
      "Epoch: 43, Batch: 11, Loss: -1.1225, Gradient Norm: 1.5946, Minimal Ratio: 0.0011\n",
      "Epoch: 43, Batch: 21, Loss: -10.1756, Gradient Norm: 1.4747, Minimal Ratio: 0.0018\n",
      "Epoch: 43, Batch: 31, Loss: -5.7775, Gradient Norm: 2.1347, Minimal Ratio: 0.0011\n",
      "Epoch: 44, Batch: 1, Loss: -10.5424, Gradient Norm: 5.2815, Minimal Ratio: 0.0007\n",
      "Epoch: 44, Batch: 11, Loss: 1.0539, Gradient Norm: 4.0515, Minimal Ratio: 0.0010\n",
      "Epoch: 44, Batch: 21, Loss: -7.5562, Gradient Norm: 3.6517, Minimal Ratio: 0.0004\n",
      "Epoch: 44, Batch: 31, Loss: -8.9982, Gradient Norm: 6.2579, Minimal Ratio: 0.0008\n",
      "Epoch: 45, Batch: 1, Loss: -4.6013, Gradient Norm: 1.8992, Minimal Ratio: 0.0010\n",
      "Epoch: 45, Batch: 11, Loss: -2.8029, Gradient Norm: 5.4566, Minimal Ratio: 0.0003\n",
      "Epoch: 45, Batch: 21, Loss: -5.7749, Gradient Norm: 4.3427, Minimal Ratio: 0.0005\n",
      "Epoch: 45, Batch: 31, Loss: 0.2555, Gradient Norm: 3.8850, Minimal Ratio: 0.0015\n",
      "Epoch: 46, Batch: 1, Loss: -5.5801, Gradient Norm: 2.7927, Minimal Ratio: 0.0040\n",
      "Epoch: 46, Batch: 11, Loss: -5.6808, Gradient Norm: 5.2422, Minimal Ratio: 0.0003\n",
      "Epoch: 46, Batch: 21, Loss: 1.0542, Gradient Norm: 1.9565, Minimal Ratio: 0.0041\n",
      "Epoch: 46, Batch: 31, Loss: -4.3485, Gradient Norm: 1.6936, Minimal Ratio: 0.0014\n",
      "Epoch: 47, Batch: 1, Loss: -3.4573, Gradient Norm: 1.9708, Minimal Ratio: 0.0029\n",
      "Epoch: 47, Batch: 11, Loss: -0.5217, Gradient Norm: 4.0952, Minimal Ratio: 0.0004\n",
      "Epoch: 47, Batch: 21, Loss: -4.0040, Gradient Norm: 3.4366, Minimal Ratio: 0.0004\n",
      "Epoch: 47, Batch: 31, Loss: -3.5160, Gradient Norm: 2.0109, Minimal Ratio: 0.0010\n",
      "Epoch: 48, Batch: 1, Loss: -5.4300, Gradient Norm: 1.7317, Minimal Ratio: 0.0011\n",
      "Epoch: 48, Batch: 11, Loss: -1.4533, Gradient Norm: 0.6743, Minimal Ratio: 0.0028\n",
      "Epoch: 48, Batch: 21, Loss: 2.1062, Gradient Norm: 3.5100, Minimal Ratio: 0.0006\n",
      "Epoch: 48, Batch: 31, Loss: -5.9504, Gradient Norm: 3.6919, Minimal Ratio: 0.0006\n",
      "Epoch: 49, Batch: 1, Loss: -7.0571, Gradient Norm: 2.3020, Minimal Ratio: 0.0007\n",
      "Epoch: 49, Batch: 11, Loss: 0.2517, Gradient Norm: 5.1149, Minimal Ratio: 0.0006\n",
      "Epoch: 49, Batch: 21, Loss: 5.9204, Gradient Norm: 4.6131, Minimal Ratio: 0.0014\n",
      "Epoch: 49, Batch: 31, Loss: 3.0778, Gradient Norm: 0.8824, Minimal Ratio: 0.0024\n",
      "Epoch: 50, Batch: 1, Loss: -8.4719, Gradient Norm: 1.8375, Minimal Ratio: 0.0014\n",
      "Epoch: 50, Batch: 11, Loss: 4.2423, Gradient Norm: 2.5601, Minimal Ratio: 0.0019\n",
      "Epoch: 50, Batch: 21, Loss: 1.2717, Gradient Norm: 2.0215, Minimal Ratio: 0.0013\n",
      "Epoch: 50, Batch: 31, Loss: 0.2976, Gradient Norm: 4.0008, Minimal Ratio: 0.0004\n",
      "Epoch: 51, Batch: 1, Loss: -1.6944, Gradient Norm: 3.2904, Minimal Ratio: 0.0006\n",
      "Epoch: 51, Batch: 11, Loss: -9.6677, Gradient Norm: 2.2273, Minimal Ratio: 0.0009\n",
      "Epoch: 51, Batch: 21, Loss: -1.1982, Gradient Norm: 2.6317, Minimal Ratio: 0.0023\n",
      "Epoch: 51, Batch: 31, Loss: -4.7921, Gradient Norm: 4.1120, Minimal Ratio: 0.0004\n",
      "Epoch: 52, Batch: 1, Loss: 0.5664, Gradient Norm: 2.1984, Minimal Ratio: 0.0005\n",
      "Epoch: 52, Batch: 11, Loss: -4.7265, Gradient Norm: 6.0314, Minimal Ratio: 0.0003\n",
      "Epoch: 52, Batch: 21, Loss: -7.0954, Gradient Norm: 4.1442, Minimal Ratio: 0.0005\n",
      "Epoch: 52, Batch: 31, Loss: -3.0290, Gradient Norm: 3.0933, Minimal Ratio: 0.0016\n",
      "Epoch: 53, Batch: 1, Loss: -9.8638, Gradient Norm: 5.8378, Minimal Ratio: 0.0007\n",
      "Epoch: 53, Batch: 11, Loss: -3.4840, Gradient Norm: 6.3673, Minimal Ratio: 0.0006\n",
      "Epoch: 53, Batch: 21, Loss: -1.6917, Gradient Norm: 3.2280, Minimal Ratio: 0.0023\n",
      "Epoch: 53, Batch: 31, Loss: -2.0437, Gradient Norm: 2.9391, Minimal Ratio: 0.0009\n",
      "Epoch: 54, Batch: 1, Loss: 4.2100, Gradient Norm: 2.7570, Minimal Ratio: 0.0010\n",
      "Epoch: 54, Batch: 11, Loss: -0.2906, Gradient Norm: 9.4623, Minimal Ratio: 0.0002\n",
      "Epoch: 54, Batch: 21, Loss: 3.5335, Gradient Norm: 4.4731, Minimal Ratio: 0.0003\n",
      "Epoch: 54, Batch: 31, Loss: -11.0944, Gradient Norm: 6.1224, Minimal Ratio: 0.0003\n",
      "Epoch: 55, Batch: 1, Loss: 0.3720, Gradient Norm: 1.4322, Minimal Ratio: 0.0019\n",
      "Epoch: 55, Batch: 11, Loss: 1.5513, Gradient Norm: 5.2764, Minimal Ratio: 0.0002\n",
      "Epoch: 55, Batch: 21, Loss: -2.1936, Gradient Norm: 3.2893, Minimal Ratio: 0.0006\n",
      "Epoch: 55, Batch: 31, Loss: -8.3304, Gradient Norm: 2.4337, Minimal Ratio: 0.0012\n",
      "Epoch: 56, Batch: 1, Loss: -6.8747, Gradient Norm: 1.2374, Minimal Ratio: 0.0011\n",
      "Epoch: 56, Batch: 11, Loss: -3.1196, Gradient Norm: 2.7933, Minimal Ratio: 0.0006\n",
      "Epoch: 56, Batch: 21, Loss: 0.1686, Gradient Norm: 2.4858, Minimal Ratio: 0.0019\n",
      "Epoch: 56, Batch: 31, Loss: 1.9378, Gradient Norm: 2.5523, Minimal Ratio: 0.0007\n",
      "Epoch: 57, Batch: 1, Loss: -4.4835, Gradient Norm: 4.3389, Minimal Ratio: 0.0004\n",
      "Epoch: 57, Batch: 11, Loss: 6.3041, Gradient Norm: 1.9009, Minimal Ratio: 0.0017\n",
      "Epoch: 57, Batch: 21, Loss: 0.4097, Gradient Norm: 6.1042, Minimal Ratio: 0.0002\n",
      "Epoch: 57, Batch: 31, Loss: -0.3936, Gradient Norm: 2.3672, Minimal Ratio: 0.0008\n",
      "Epoch: 58, Batch: 1, Loss: -3.4609, Gradient Norm: 3.4109, Minimal Ratio: 0.0004\n",
      "Epoch: 58, Batch: 11, Loss: -3.3263, Gradient Norm: 3.6908, Minimal Ratio: 0.0004\n",
      "Epoch: 58, Batch: 21, Loss: 0.2360, Gradient Norm: 5.8003, Minimal Ratio: 0.0004\n",
      "Epoch: 58, Batch: 31, Loss: -1.4050, Gradient Norm: 1.8659, Minimal Ratio: 0.0015\n",
      "Epoch: 59, Batch: 1, Loss: -5.4724, Gradient Norm: 2.0349, Minimal Ratio: 0.0006\n",
      "Epoch: 59, Batch: 11, Loss: -2.4726, Gradient Norm: 2.5196, Minimal Ratio: 0.0006\n",
      "Epoch: 59, Batch: 21, Loss: -6.0559, Gradient Norm: 2.1278, Minimal Ratio: 0.0009\n",
      "Epoch: 59, Batch: 31, Loss: -3.1667, Gradient Norm: 1.2864, Minimal Ratio: 0.0011\n",
      "Epoch: 60, Batch: 1, Loss: -0.6304, Gradient Norm: 3.4096, Minimal Ratio: 0.0004\n",
      "Epoch: 60, Batch: 11, Loss: 0.8368, Gradient Norm: 3.2839, Minimal Ratio: 0.0003\n",
      "Epoch: 60, Batch: 21, Loss: 3.1503, Gradient Norm: 11.4620, Minimal Ratio: 0.0001\n",
      "Epoch: 60, Batch: 31, Loss: 3.1169, Gradient Norm: 5.9341, Minimal Ratio: 0.0021\n",
      "Epoch: 61, Batch: 1, Loss: -6.6132, Gradient Norm: 2.0350, Minimal Ratio: 0.0108\n",
      "Epoch: 61, Batch: 11, Loss: -5.8989, Gradient Norm: 3.8016, Minimal Ratio: 0.0003\n",
      "Epoch: 61, Batch: 21, Loss: -9.6434, Gradient Norm: 2.7890, Minimal Ratio: 0.0010\n",
      "Epoch: 61, Batch: 31, Loss: -6.5415, Gradient Norm: 11.2499, Minimal Ratio: 0.0001\n",
      "Epoch: 62, Batch: 1, Loss: -3.2958, Gradient Norm: 6.2004, Minimal Ratio: 0.0002\n",
      "Epoch: 62, Batch: 11, Loss: 4.1708, Gradient Norm: 4.0545, Minimal Ratio: 0.0007\n",
      "Epoch: 62, Batch: 21, Loss: -3.2955, Gradient Norm: 2.3091, Minimal Ratio: 0.0008\n",
      "Epoch: 62, Batch: 31, Loss: -5.5893, Gradient Norm: 2.6863, Minimal Ratio: 0.0007\n",
      "Epoch: 63, Batch: 1, Loss: -2.3168, Gradient Norm: 4.3808, Minimal Ratio: 0.0002\n",
      "Epoch: 63, Batch: 11, Loss: 1.1035, Gradient Norm: 4.5149, Minimal Ratio: 0.0003\n",
      "Epoch: 63, Batch: 21, Loss: -16.4492, Gradient Norm: 5.2760, Minimal Ratio: 0.0004\n",
      "Epoch: 63, Batch: 31, Loss: -0.8027, Gradient Norm: 4.9148, Minimal Ratio: 0.0002\n",
      "Epoch: 64, Batch: 1, Loss: 1.6860, Gradient Norm: 6.1750, Minimal Ratio: 0.0002\n",
      "Epoch: 64, Batch: 11, Loss: -6.8112, Gradient Norm: 7.4925, Minimal Ratio: 0.0002\n",
      "Epoch: 64, Batch: 21, Loss: -7.2429, Gradient Norm: 4.6300, Minimal Ratio: 0.0010\n",
      "Epoch: 64, Batch: 31, Loss: -4.9549, Gradient Norm: 8.2965, Minimal Ratio: 0.0002\n",
      "Epoch: 65, Batch: 1, Loss: -2.1546, Gradient Norm: 4.0921, Minimal Ratio: 0.0004\n",
      "Epoch: 65, Batch: 11, Loss: -2.0256, Gradient Norm: 4.6030, Minimal Ratio: 0.0005\n",
      "Epoch: 65, Batch: 21, Loss: -0.5621, Gradient Norm: 5.5610, Minimal Ratio: 0.0002\n",
      "Epoch: 65, Batch: 31, Loss: -4.2404, Gradient Norm: 2.9474, Minimal Ratio: 0.0005\n",
      "Epoch: 66, Batch: 1, Loss: -3.4772, Gradient Norm: 3.7382, Minimal Ratio: 0.0004\n",
      "Epoch: 66, Batch: 11, Loss: 1.5533, Gradient Norm: 4.9237, Minimal Ratio: 0.0023\n",
      "Epoch: 66, Batch: 21, Loss: 4.3210, Gradient Norm: 12.2126, Minimal Ratio: 0.0001\n",
      "Epoch: 66, Batch: 31, Loss: -3.8627, Gradient Norm: 2.3100, Minimal Ratio: 0.0026\n",
      "Epoch: 67, Batch: 1, Loss: -0.6347, Gradient Norm: 3.0438, Minimal Ratio: 0.0003\n",
      "Epoch: 67, Batch: 11, Loss: -7.5407, Gradient Norm: 2.8269, Minimal Ratio: 0.0010\n",
      "Epoch: 67, Batch: 21, Loss: -12.3314, Gradient Norm: 4.5962, Minimal Ratio: 0.0003\n",
      "Epoch: 67, Batch: 31, Loss: 5.8153, Gradient Norm: 5.0445, Minimal Ratio: 0.0000\n",
      "Epoch: 68, Batch: 1, Loss: -9.8510, Gradient Norm: 2.5920, Minimal Ratio: 0.0012\n",
      "Epoch: 68, Batch: 11, Loss: -3.9873, Gradient Norm: 2.3862, Minimal Ratio: 0.0022\n",
      "Epoch: 68, Batch: 21, Loss: 0.5111, Gradient Norm: 3.0448, Minimal Ratio: 0.0006\n",
      "Epoch: 68, Batch: 31, Loss: -5.8060, Gradient Norm: 2.0604, Minimal Ratio: 0.0012\n",
      "Epoch: 69, Batch: 1, Loss: 4.4686, Gradient Norm: 6.8645, Minimal Ratio: 0.0004\n",
      "Epoch: 69, Batch: 11, Loss: -9.6342, Gradient Norm: 4.9068, Minimal Ratio: 0.0003\n",
      "Epoch: 69, Batch: 21, Loss: -1.4022, Gradient Norm: 3.2956, Minimal Ratio: 0.0011\n",
      "Epoch: 69, Batch: 31, Loss: 1.5434, Gradient Norm: 6.9659, Minimal Ratio: 0.0002\n",
      "Epoch: 70, Batch: 1, Loss: -8.0961, Gradient Norm: 6.5014, Minimal Ratio: 0.0005\n",
      "Epoch: 70, Batch: 11, Loss: -9.5257, Gradient Norm: 3.2388, Minimal Ratio: 0.0005\n",
      "Epoch: 70, Batch: 21, Loss: -4.6754, Gradient Norm: 6.0222, Minimal Ratio: 0.0021\n",
      "Epoch: 70, Batch: 31, Loss: -1.7352, Gradient Norm: 7.1940, Minimal Ratio: 0.0002\n",
      "Epoch: 71, Batch: 1, Loss: -10.0757, Gradient Norm: 2.5885, Minimal Ratio: 0.0007\n",
      "Epoch: 71, Batch: 11, Loss: -8.1092, Gradient Norm: 2.4642, Minimal Ratio: 0.0018\n",
      "Epoch: 71, Batch: 21, Loss: -5.7433, Gradient Norm: 2.0438, Minimal Ratio: 0.0006\n",
      "Epoch: 71, Batch: 31, Loss: -3.4848, Gradient Norm: 7.0869, Minimal Ratio: 0.0003\n",
      "Epoch: 72, Batch: 1, Loss: -5.2361, Gradient Norm: 2.8493, Minimal Ratio: 0.0009\n",
      "Epoch: 72, Batch: 11, Loss: 0.8147, Gradient Norm: 3.5712, Minimal Ratio: 0.0014\n",
      "Epoch: 72, Batch: 21, Loss: -0.0662, Gradient Norm: 4.3478, Minimal Ratio: 0.0003\n",
      "Epoch: 72, Batch: 31, Loss: -4.7554, Gradient Norm: 6.8414, Minimal Ratio: 0.0002\n",
      "Epoch: 73, Batch: 1, Loss: -8.2401, Gradient Norm: 2.5186, Minimal Ratio: 0.0038\n",
      "Epoch: 73, Batch: 11, Loss: -0.8160, Gradient Norm: 4.7462, Minimal Ratio: 0.0004\n",
      "Epoch: 73, Batch: 21, Loss: -0.7453, Gradient Norm: 10.3637, Minimal Ratio: 0.0001\n",
      "Epoch: 73, Batch: 31, Loss: -0.9034, Gradient Norm: 3.4955, Minimal Ratio: 0.0052\n",
      "Epoch: 74, Batch: 1, Loss: -0.1081, Gradient Norm: 4.1673, Minimal Ratio: 0.0004\n",
      "Epoch: 74, Batch: 11, Loss: -3.5949, Gradient Norm: 2.8703, Minimal Ratio: 0.0006\n",
      "Epoch: 74, Batch: 21, Loss: -11.0267, Gradient Norm: 4.5613, Minimal Ratio: 0.0004\n",
      "Epoch: 74, Batch: 31, Loss: -0.3387, Gradient Norm: 4.8612, Minimal Ratio: 0.0001\n",
      "Epoch: 75, Batch: 1, Loss: -6.0655, Gradient Norm: 5.1545, Minimal Ratio: 0.0003\n",
      "Epoch: 75, Batch: 11, Loss: -2.0309, Gradient Norm: 4.1350, Minimal Ratio: 0.0004\n",
      "Epoch: 75, Batch: 21, Loss: -3.4297, Gradient Norm: 5.8244, Minimal Ratio: 0.0002\n",
      "Epoch: 75, Batch: 31, Loss: -6.9067, Gradient Norm: 8.2416, Minimal Ratio: 0.0001\n",
      "Epoch: 76, Batch: 1, Loss: -9.5145, Gradient Norm: 2.2568, Minimal Ratio: 0.0005\n",
      "Epoch: 76, Batch: 11, Loss: -7.4029, Gradient Norm: 3.3981, Minimal Ratio: 0.0003\n",
      "Epoch: 76, Batch: 21, Loss: -9.3730, Gradient Norm: 3.1269, Minimal Ratio: 0.0009\n",
      "Epoch: 76, Batch: 31, Loss: -5.6193, Gradient Norm: 3.4993, Minimal Ratio: 0.0017\n",
      "Epoch: 77, Batch: 1, Loss: -5.5415, Gradient Norm: 3.6106, Minimal Ratio: 0.0004\n",
      "Epoch: 77, Batch: 11, Loss: 0.4372, Gradient Norm: 4.6436, Minimal Ratio: 0.0004\n",
      "Epoch: 77, Batch: 21, Loss: -1.6617, Gradient Norm: 4.5696, Minimal Ratio: 0.0002\n",
      "Epoch: 77, Batch: 31, Loss: -8.7671, Gradient Norm: 3.5715, Minimal Ratio: 0.0016\n",
      "Epoch: 78, Batch: 1, Loss: -6.3873, Gradient Norm: 6.9805, Minimal Ratio: 0.0003\n",
      "Epoch: 78, Batch: 11, Loss: -0.4078, Gradient Norm: 3.1757, Minimal Ratio: 0.0008\n",
      "Epoch: 78, Batch: 21, Loss: -15.3090, Gradient Norm: 3.9077, Minimal Ratio: 0.0006\n",
      "Epoch: 78, Batch: 31, Loss: 0.9237, Gradient Norm: 4.4249, Minimal Ratio: 0.0003\n",
      "Epoch: 79, Batch: 1, Loss: -10.0579, Gradient Norm: 3.4459, Minimal Ratio: 0.0004\n",
      "Epoch: 79, Batch: 11, Loss: -7.3449, Gradient Norm: 5.3619, Minimal Ratio: 0.0002\n",
      "Epoch: 79, Batch: 21, Loss: -1.1577, Gradient Norm: 5.1316, Minimal Ratio: 0.0002\n",
      "Epoch: 79, Batch: 31, Loss: -7.7788, Gradient Norm: 1.8186, Minimal Ratio: 0.0015\n",
      "Epoch: 80, Batch: 1, Loss: -6.0594, Gradient Norm: 3.5120, Minimal Ratio: 0.0007\n",
      "Epoch: 80, Batch: 11, Loss: -10.8613, Gradient Norm: 4.6709, Minimal Ratio: 0.0005\n",
      "Epoch: 80, Batch: 21, Loss: -3.6693, Gradient Norm: 4.2862, Minimal Ratio: 0.0007\n",
      "Epoch: 80, Batch: 31, Loss: -8.8729, Gradient Norm: 6.9734, Minimal Ratio: 0.0002\n",
      "Epoch: 81, Batch: 1, Loss: -2.3156, Gradient Norm: 1.9636, Minimal Ratio: 0.0011\n",
      "Epoch: 81, Batch: 11, Loss: 0.6608, Gradient Norm: 8.1714, Minimal Ratio: 0.0002\n",
      "Epoch: 81, Batch: 21, Loss: -5.9894, Gradient Norm: 9.4113, Minimal Ratio: 0.0001\n",
      "Epoch: 81, Batch: 31, Loss: -5.9487, Gradient Norm: 3.2274, Minimal Ratio: 0.0012\n",
      "Epoch: 82, Batch: 1, Loss: -6.5585, Gradient Norm: 5.7760, Minimal Ratio: 0.0003\n",
      "Epoch: 82, Batch: 11, Loss: -9.5401, Gradient Norm: 4.4919, Minimal Ratio: 0.0003\n",
      "Epoch: 82, Batch: 21, Loss: -4.7066, Gradient Norm: 4.2876, Minimal Ratio: 0.0003\n",
      "Epoch: 82, Batch: 31, Loss: -2.1630, Gradient Norm: 7.7814, Minimal Ratio: 0.0003\n",
      "Epoch: 83, Batch: 1, Loss: -8.2347, Gradient Norm: 4.0448, Minimal Ratio: 0.0017\n",
      "Epoch: 83, Batch: 11, Loss: -4.4724, Gradient Norm: 1.3214, Minimal Ratio: 0.0018\n",
      "Epoch: 83, Batch: 21, Loss: -9.3753, Gradient Norm: 5.0445, Minimal Ratio: 0.0003\n",
      "Epoch: 83, Batch: 31, Loss: -1.7753, Gradient Norm: 4.6496, Minimal Ratio: 0.0004\n",
      "Epoch: 84, Batch: 1, Loss: -1.3475, Gradient Norm: 3.2529, Minimal Ratio: 0.0007\n",
      "Epoch: 84, Batch: 11, Loss: 9.1188, Gradient Norm: 9.4471, Minimal Ratio: 0.0002\n",
      "Epoch: 84, Batch: 21, Loss: -5.6601, Gradient Norm: 3.8627, Minimal Ratio: 0.0004\n",
      "Epoch: 84, Batch: 31, Loss: -7.1231, Gradient Norm: 4.3134, Minimal Ratio: 0.0006\n",
      "Epoch: 85, Batch: 1, Loss: 0.8346, Gradient Norm: 4.1269, Minimal Ratio: 0.0008\n",
      "Epoch: 85, Batch: 11, Loss: -6.8995, Gradient Norm: 5.9030, Minimal Ratio: 0.0004\n",
      "Epoch: 85, Batch: 21, Loss: -5.9402, Gradient Norm: 5.3731, Minimal Ratio: 0.0006\n",
      "Epoch: 85, Batch: 31, Loss: -9.9786, Gradient Norm: 9.1353, Minimal Ratio: 0.0001\n",
      "Epoch: 86, Batch: 1, Loss: -3.9020, Gradient Norm: 2.3363, Minimal Ratio: 0.0009\n",
      "Epoch: 86, Batch: 11, Loss: 1.0925, Gradient Norm: 7.4119, Minimal Ratio: 0.0002\n",
      "Epoch: 86, Batch: 21, Loss: -6.8100, Gradient Norm: 3.5408, Minimal Ratio: 0.0003\n",
      "Epoch: 86, Batch: 31, Loss: -8.6665, Gradient Norm: 6.6965, Minimal Ratio: 0.0003\n",
      "Epoch: 87, Batch: 1, Loss: -4.1382, Gradient Norm: 9.8440, Minimal Ratio: 0.0002\n",
      "Epoch: 87, Batch: 11, Loss: -11.4239, Gradient Norm: 2.0029, Minimal Ratio: 0.0015\n",
      "Epoch: 87, Batch: 21, Loss: -9.9220, Gradient Norm: 6.4970, Minimal Ratio: 0.0004\n",
      "Epoch: 87, Batch: 31, Loss: -5.9218, Gradient Norm: 4.0086, Minimal Ratio: 0.0007\n",
      "Epoch: 88, Batch: 1, Loss: -0.0851, Gradient Norm: 7.6785, Minimal Ratio: 0.0003\n",
      "Epoch: 88, Batch: 11, Loss: -0.5924, Gradient Norm: 3.0264, Minimal Ratio: 0.0005\n",
      "Epoch: 88, Batch: 21, Loss: 2.2498, Gradient Norm: 4.2689, Minimal Ratio: 0.0009\n",
      "Epoch: 88, Batch: 31, Loss: 7.1413, Gradient Norm: 3.3874, Minimal Ratio: 0.0011\n",
      "Epoch: 89, Batch: 1, Loss: 0.3599, Gradient Norm: 4.2791, Minimal Ratio: 0.0004\n",
      "Epoch: 89, Batch: 11, Loss: 4.3689, Gradient Norm: 4.1518, Minimal Ratio: 0.0008\n",
      "Epoch: 89, Batch: 21, Loss: -6.9532, Gradient Norm: 4.3552, Minimal Ratio: 0.0003\n",
      "Epoch: 89, Batch: 31, Loss: -5.5172, Gradient Norm: 6.0947, Minimal Ratio: 0.0005\n",
      "Epoch: 90, Batch: 1, Loss: -7.5449, Gradient Norm: 4.3911, Minimal Ratio: 0.0001\n",
      "Epoch: 90, Batch: 11, Loss: -2.5996, Gradient Norm: 4.5768, Minimal Ratio: 0.0008\n",
      "Epoch: 90, Batch: 21, Loss: -6.7127, Gradient Norm: 2.9415, Minimal Ratio: 0.0009\n",
      "Epoch: 90, Batch: 31, Loss: -0.6761, Gradient Norm: 4.5769, Minimal Ratio: 0.0004\n",
      "Epoch: 91, Batch: 1, Loss: -3.0789, Gradient Norm: 3.2000, Minimal Ratio: 0.0010\n",
      "Epoch: 91, Batch: 11, Loss: -10.8492, Gradient Norm: 4.3472, Minimal Ratio: 0.0007\n",
      "Epoch: 91, Batch: 21, Loss: 0.1129, Gradient Norm: 6.2693, Minimal Ratio: 0.0005\n",
      "Epoch: 91, Batch: 31, Loss: -8.3294, Gradient Norm: 10.4051, Minimal Ratio: 0.0003\n",
      "Epoch: 92, Batch: 1, Loss: -6.1379, Gradient Norm: 4.0042, Minimal Ratio: 0.0011\n",
      "Epoch: 92, Batch: 11, Loss: -1.1518, Gradient Norm: 6.1552, Minimal Ratio: 0.0002\n",
      "Epoch: 92, Batch: 21, Loss: -0.7361, Gradient Norm: 6.1597, Minimal Ratio: 0.0002\n",
      "Epoch: 92, Batch: 31, Loss: -10.8783, Gradient Norm: 8.5092, Minimal Ratio: 0.0001\n",
      "Epoch: 93, Batch: 1, Loss: -2.5402, Gradient Norm: 2.9319, Minimal Ratio: 0.0006\n",
      "Epoch: 93, Batch: 11, Loss: -1.3163, Gradient Norm: 2.2511, Minimal Ratio: 0.0003\n",
      "Epoch: 93, Batch: 21, Loss: -10.5695, Gradient Norm: 5.9538, Minimal Ratio: 0.0004\n",
      "Epoch: 93, Batch: 31, Loss: -3.1810, Gradient Norm: 3.5257, Minimal Ratio: 0.0006\n",
      "Epoch: 94, Batch: 1, Loss: -4.3825, Gradient Norm: 2.2272, Minimal Ratio: 0.0015\n",
      "Epoch: 94, Batch: 11, Loss: -8.2577, Gradient Norm: 9.6066, Minimal Ratio: 0.0005\n",
      "Epoch: 94, Batch: 21, Loss: -4.5187, Gradient Norm: 3.2821, Minimal Ratio: 0.0006\n",
      "Epoch: 94, Batch: 31, Loss: -3.7593, Gradient Norm: 7.2204, Minimal Ratio: 0.0013\n",
      "Epoch: 95, Batch: 1, Loss: -6.8217, Gradient Norm: 3.9818, Minimal Ratio: 0.0017\n",
      "Epoch: 95, Batch: 11, Loss: -4.1739, Gradient Norm: 4.8029, Minimal Ratio: 0.0012\n",
      "Epoch: 95, Batch: 21, Loss: -4.1926, Gradient Norm: 5.3241, Minimal Ratio: 0.0003\n",
      "Epoch: 95, Batch: 31, Loss: -1.2752, Gradient Norm: 2.1421, Minimal Ratio: 0.0017\n",
      "Epoch: 96, Batch: 1, Loss: -2.3105, Gradient Norm: 8.2641, Minimal Ratio: 0.0002\n",
      "Epoch: 96, Batch: 11, Loss: -7.5985, Gradient Norm: 4.5126, Minimal Ratio: 0.0004\n",
      "Epoch: 96, Batch: 21, Loss: -3.3451, Gradient Norm: 10.9992, Minimal Ratio: 0.0001\n",
      "Epoch: 96, Batch: 31, Loss: 5.8977, Gradient Norm: 8.7557, Minimal Ratio: 0.0003\n",
      "Epoch: 97, Batch: 1, Loss: -4.1020, Gradient Norm: 3.0380, Minimal Ratio: 0.0007\n",
      "Epoch: 97, Batch: 11, Loss: -7.5880, Gradient Norm: 3.5885, Minimal Ratio: 0.0006\n",
      "Epoch: 97, Batch: 21, Loss: -5.7540, Gradient Norm: 6.0464, Minimal Ratio: 0.0004\n",
      "Epoch: 97, Batch: 31, Loss: -6.2779, Gradient Norm: 3.9004, Minimal Ratio: 0.0010\n",
      "Epoch: 98, Batch: 1, Loss: -2.5295, Gradient Norm: 7.3199, Minimal Ratio: 0.0002\n",
      "Epoch: 98, Batch: 11, Loss: -4.1386, Gradient Norm: 4.5522, Minimal Ratio: 0.0006\n",
      "Epoch: 98, Batch: 21, Loss: -4.6695, Gradient Norm: 4.5217, Minimal Ratio: 0.0005\n",
      "Epoch: 98, Batch: 31, Loss: -11.9355, Gradient Norm: 4.6564, Minimal Ratio: 0.0010\n",
      "Epoch: 99, Batch: 1, Loss: -4.2548, Gradient Norm: 2.3316, Minimal Ratio: 0.0007\n",
      "Epoch: 99, Batch: 11, Loss: -5.8575, Gradient Norm: 3.8532, Minimal Ratio: 0.0005\n",
      "Epoch: 99, Batch: 21, Loss: 3.7220, Gradient Norm: 4.2648, Minimal Ratio: 0.0004\n",
      "Epoch: 99, Batch: 31, Loss: -5.8310, Gradient Norm: 3.1614, Minimal Ratio: 0.0007\n",
      "Epoch: 100, Batch: 1, Loss: 2.2570, Gradient Norm: 2.6083, Minimal Ratio: 0.0009\n",
      "Epoch: 100, Batch: 11, Loss: -6.8320, Gradient Norm: 7.6673, Minimal Ratio: 0.0002\n",
      "Epoch: 100, Batch: 21, Loss: -7.3107, Gradient Norm: 17.9059, Minimal Ratio: 0.0001\n",
      "Epoch: 100, Batch: 31, Loss: -6.8946, Gradient Norm: 6.1424, Minimal Ratio: 0.0006\n"
     ]
    }
   ],
   "source": [
    "minimal_ratios = []\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Gradient norm\n",
    "        gradient_norm = 0\n",
    "        for param in model.parameters():\n",
    "            gradient_norm += torch.sum(param.grad ** 2)\n",
    "        gradient_norm = torch.sqrt(gradient_norm)\n",
    "        \n",
    "        # modify function\n",
    "        new_loss = loss + 0.001 * gradient_norm\n",
    "        \n",
    "        # min ratio\n",
    "        minimal_ratio = float('inf')\n",
    "        for param in model.parameters():\n",
    "            ratio = torch.abs(param).min().cpu().detach().numpy() / torch.abs(param.grad).max().cpu().detach().numpy()\n",
    "            if ratio < minimal_ratio:\n",
    "                minimal_ratio = ratio\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item():.4f}, Gradient Norm: {gradient_norm.item():.4f}, Minimal Ratio: {minimal_ratio:.4f}\")\n",
    "            \n",
    "            # Store the minimal ratio and loss values in the lists\n",
    "            minimal_ratios.append(minimal_ratio)\n",
    "            losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dea3e8ad-d716-41db-b8ca-661794b47f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plt.scatter(losses, minimal_ratios, c='blue')\n",
    "plt.xlabel('Minimal Ratio')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Minimal Ratio vs. Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122485af-e9da-431e-92b5-e6a5ea5c2356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ba653-7377-4c66-b7a5-8d85357e340f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
